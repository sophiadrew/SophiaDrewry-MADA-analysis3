---
title: "Analysis3"
author: "Sophia Drewry"
date: "11/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This script loads the cleaned and processed data to perform some formal statistical fitting
This excersise will focus on the continious outcome "BodyTemp"
```{r}
# load needed packages. make sure they are installed or else...
library(dplyr) #for data processing
library(here) #to set paths
library(tidymodels) #to fit models
library(rpart)
library(glmnet)
library(ranger)
library(future)

```
#Load data
```{r}
# note the use of the here() package and not absolute paths
dataSPOT <- here::here("data","processed_data","processeddta.rds")
# load data. 
processeddta <-readRDS(dataSPOT)
```
# Data splitting
Here we are going to split the data randomly into training and testing subsets
- Training data will be used to fit the model. 
- Testing set will be used to evaluate the model.
```{r}
# Setting a seed for random number generation so if this analysis is reproduced, the same random set will be generated
set.seed(123)
# Subsetting 70% of data into training and 20% of data into testing
# We using Body Temp to stratify
data_split <- initial_split(processeddta, prop = .7, strata = BodyTemp)
# Creating training data
train_data <- training(data_split)
# Creating testing data
test_data  <- testing(data_split)
```
# 5-fold cross-validation, 5x repeated
```{r}
# Creating a resample object for our trainng data
set.seed(123)
folds <- vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)
folds
```
///////////////////////////////////////////////////////////////////////////////
## Setting workflows & training models: Model 1 
Setting up lr.mod that will be used for the rest of the excersise
```{r}
lr.mod <-  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
```
###### Not sure if I need to create this for comparison...
```{r}
#Setting up the linear model
lr.mod <-  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
#recipe
BodyTemp.rec <- recipe(BodyTemp ~ ., data = train_data)
# Regular model workflow
BodyTemp.wflow <-
  workflow() %>% 
  add_model(lr.mod) %>% 
  add_recipe(BodyTemp.rec)
# Regular model training
BodyTemp.fit <- 
  BodyTemp.wflow %>% 
  fit(data = train_data)
# To view a tibble 
BodyTemp.fit %>%
  extract_fit_parsnip() %>%
  tidy()
# paUSING FOR NOW BECAUSE I DONT THINK I NEED THIS, DELETE
```

# Fitting a linear model to BodyTemp 
```{r}
#########################  Dummy Var #########################
# Creating Recipe TRAIN DTA for all categorical Dummy Variables
#Setting up the linear model
D.BodyTemp.rec <- recipe(BodyTemp ~ ., data = train_data)  %>% 
  step_dummy(all_nominal(), -BodyTemp)
# Create workflow
D.BT.wflow <- workflow() %>% 
  add_model(lr.mod) %>% 
  add_recipe(BodyTemp.rec)
# Fit model to training data
D.BT.fit <- 
  D.BT.wflow %>% 
  fit(data = train_data)
# evaluate
D.BT.fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

# Creating a Null Model 
```{r}
# Create null formula
N.BodyTemp.rec <- recipe(BodyTemp ~ 1., data = train_data)  
```

#########################  Null Training    #########################
```{r}
# Creating null recipe & model with TRAIN data
# set workflow
N.BT.train.wflow <-
  workflow() %>% 
  add_model(lr.mod) %>% 
  add_recipe(N.BodyTemp.rec)
# fitting
N.BT.train.fit <- 
  N.BT.train.wflow %>% 
  fit(data = train_data)
# usual
N.BT.train.fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
# RMSE
predict(N.BT.train.fit, train_data)
N.BT.train.aug <- augment(N.BT.train.fit, train_data)
N.BT.train.aug %>% select(BodyTemp, .pred) 
N.BT.train.aug %>% rmse(truth = BodyTemp, .pred)

# RMSE = 1.209327	
```
Notes: I got same RMSE using null_model() and also manually setting the formula
# No idea what to do with folds... kept just in case
N.BT.train.fit <- fit_resamples(N.BT.train.wflow, resamples = folds)
 N.BT.train.rmse <- collect_metrics(N.BT.train.fit)
# broken, saved just in case
N.mod<- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("regression")
  
################################ Null  Testing   ################################
```{r}
# fitting
N.BT.train.fit <- 
  N.BT.train.wflow %>% 
  fit(data = train_data)
predict(N.BT.train.fit, test_data)
N.BT.train.aug <- augment(N.BT.train.fit, test_data) # I dont think i need this
N.BT.train.aug %>% select(BodyTemp, .pred) 
N.BT.train.aug %>% #taking the root-mean square error of the model
  rmse(truth = BodyTemp, .pred)
# RMSE = 1.163343	

```
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
# Creating Models
################################   Tree  ################################
```{r}
## Tuning hyperparameters
tune_spec <- 
  decision_tree(cost_complexity = tune(), tree_depth = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
tune_spec # We will come back to these parameters
## Create a grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid %>% count(tree_depth)
# creating CV folds
set.seed(123)
train.folds <- vfold_cv(train_data)
```
## Tuning with a grid
```{r}
treeBT <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(BodyTemp.rec)
tree_res <- treeBT %>% 
  tune_grid(resamples = train.folds, grid = tree_grid)
tree_res %>% collect_metrics()
```
## Plotting for the world to see
```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
# Looks like we have 2 deeper "trees" that perform similar in cost complexity as well, but not the best
# Lets check out the top 5
tree_res %>% show_best("rmse")
# Now to pull out the best set of hyperparameter values for our decision tree model
best_tree <- tree_res %>% select_best("rmse")
best_tree
```
## Finalize the model
```{r}
# finalize workflow
final_wf <- treeBT %>% finalize_workflow(best_tree)
# final fit
final_fit <- final_wf %>% last_fit(data_split) 
final_fit %>% collect_metrics()
final_fit %>% collect_predictions() 

# RMSE = 1.1874943427
```

## Visualize
```{r}
final_tree <- extract_workflow(final_fit)
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```
################################   LASSO  ################################
sources:
https://www.tidymodels.org/start/case-study/
https://stackoverflow.com/questions/66639452/tuning-a-lasso-model-and-predicting-using-tidymodels

## Building model
```{r}
# set workflow
lasso.mod <- linear_reg(mode = "regression", penalty = tune(), mixture = 1) %>% 
   set_engine("glmnet")
lasso.wflow <- workflow() %>%
    add_model(lasso.mod) %>%
    add_recipe(BodyTemp.rec)
```
## Train and tune LASSO
```{r}
# creating grid and tuning
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30)) # I have no idea what this is
lr_reg_grid %>% top_n(-5)
lr_reg_grid %>% top_n(5)

lasso.res <- lasso.wflow %>% 
  tune_grid(resamples = train.folds,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

```
Once again, I am unable to visualize 
lasso.plot <- lasso.res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  scale_x_log10(labels = scales::label_number())
lasso.plot 
## Visulaize
```{r}
lasso.top.models <- lasso.res %>% 
  show_best("rmse", n = 15) 
lasso.top.models

lasso.best <- 
  lasso.res %>% 
  collect_metrics() %>% 
  slice(12)
lasso.best
```
### failure, but saved for just in case
lasso.auc <- 
  lasso.res %>% 
  collect_predictions(parameters = lasso.best) %>% 
  rmse(BodyTemp, .predictions) %>% # what goes here instead of .predictionss?
  mutate(model = "Linear Regression")
autoplot(lasso.auc)

```{r}
# LASSO ATTEMPT #2
lasso_rec <- recipe(BodyTemp ~ ., data = train_data) %>% 
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes())

lasso_prep <- lasso_rec %>%
  prep(strings_as_factors = FALSE)

lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

lasso_wf <- workflow() %>%
  add_recipe(lasso_rec)

lasso_fit <- lasso_wf %>%
  add_model(lasso_spec) %>%
  fit(data = train_data)

lasso_fit %>%
  pull_workflow_fit() %>%
  tidy()


```




################################   Random Forrest  ################################
sources:
https://www.tidymodels.org/start/case-study/
https://stackoverflow.com/questions/65370000/tidymodels-a-plot-showing-performance-model-metrics-rmse-rsq-for-a-random-f
```{r}
# query the number of cores on my own computer
cores <- parallel::detectCores()
cores

rf.mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```
## Create recipe and workflow
```{r}
rf.recipe <- recipe(BodyTemp ~ ., data = train_data)  %>% 
  step_dummy(all_nominal(), -BodyTemp)
rf.wflow <- 
  workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rf.recipe)
```
# Train and tune
```{r}
rf.mod
rf.mod %>% parameters() 
# space-filling design to tune, with 25 candidate models
set.seed(123)
rf.res <- rf.wflow %>% 
  tune_grid(resamples = folds, grid = 9,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# R keeps aborting session...
```

```{r}
rf_res %>% show_best(metric = "rmse")
autoplot(rf_res)

rf_best <- rf_res %>% 
  select_best(metric = "rmse")
rf_best

rf_res %>% 
  collect_predictions()
```

Attempt #2 for Trees
```{r}
set.seed(123)
mod_rf <-rand_forest() %>%
  set_engine("ranger",
             num.threads = parallel::detectCores(), 
             importance = "permutation", 
             verbose = TRUE) %>% 
  set_mode("regression") %>% 
  set_args(trees = 100)
```

```{r}
rf.recipe <- recipe(BodyTemp ~ ., data = train_data)  %>% 
  step_dummy(all_nominal(), -BodyTemp)
rf.wflow <- 
  workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rf.recipe)

```

```{r}
set.seed(123)
plan(multisession)

fit_rf <- fit_resamples(
  rf.wflow,
  resamples = folds,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)

```
library(future)


```
